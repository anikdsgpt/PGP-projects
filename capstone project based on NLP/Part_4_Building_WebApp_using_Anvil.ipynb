{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part_4_Building_WebApp_using_Anvil.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q3hhv3OmEBKM"
      },
      "source": [
        "### Part_4_Building_WebApp_using_Anvil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OG-fXkz6U6tT",
        "outputId": "d1431b39-5647-4901-8aac-28444c6f2871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        " # Mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BbNE4ZBfU9UY",
        "outputId": "f649491a-aa62-46c5-c16a-1ac325ef5769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Change the current working Directory \n",
        "import os\n",
        "\n",
        "path = '/content/drive/My Drive/Project/NLP'\n",
        "try:   \n",
        "    os.chdir(path)\n",
        "    print(\"Directory changed to Project folder\")\n",
        "except OSError:\n",
        "    print(\"Can't change the Current Working Directory\")  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory changed to Project folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7jc9D357UWTW",
        "outputId": "514e4a86-4b14-4d5b-a5ad-d94037dd1243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import keras.backend as K\n",
        "def precision(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "STkPm_xZVmuW",
        "outputId": "ca0afc77-5438-421c-b94a-29a52b608eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "# Loading the best fit model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "dependencies = {\n",
        "    'precision' : precision,\n",
        "    'recall' : recall\n",
        "}\n",
        "\n",
        "model = load_model('model_emb_bilstm_sampledData.h5', custom_objects=dependencies)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5qrAePCWKcv",
        "outputId": "533d665c-7502-4818-895e-58e0ea2f85ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "# Viewing the summary\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 926)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 926, 300)          4416600   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 926, 600)          1442400   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 926, 300)          180300    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 277800)            0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 300)               83340300  \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 187)               56287     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 74)                13912     \n",
            "=================================================================\n",
            "Total params: 89,449,799\n",
            "Trainable params: 89,449,799\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1U2jO6fi1msF",
        "colab": {}
      },
      "source": [
        "# Loading the unique labels\n",
        "import numpy as np\n",
        "labels=np.load('uniqueLabels.npy',allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gqIedy9e1mrD",
        "outputId": "96cc6c63-a856-4db4-8b13-fc1ef13e2623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['GRP_0' 'GRP_1' 'GRP_10' 'GRP_11' 'GRP_12' 'GRP_13' 'GRP_14' 'GRP_15'\n",
            " 'GRP_16' 'GRP_17' 'GRP_18' 'GRP_19' 'GRP_2' 'GRP_20' 'GRP_21' 'GRP_22'\n",
            " 'GRP_23' 'GRP_24' 'GRP_25' 'GRP_26' 'GRP_27' 'GRP_28' 'GRP_29' 'GRP_3'\n",
            " 'GRP_30' 'GRP_31' 'GRP_32' 'GRP_33' 'GRP_34' 'GRP_35' 'GRP_36' 'GRP_37'\n",
            " 'GRP_38' 'GRP_39' 'GRP_4' 'GRP_40' 'GRP_41' 'GRP_42' 'GRP_43' 'GRP_44'\n",
            " 'GRP_45' 'GRP_46' 'GRP_47' 'GRP_48' 'GRP_49' 'GRP_5' 'GRP_50' 'GRP_51'\n",
            " 'GRP_52' 'GRP_53' 'GRP_54' 'GRP_55' 'GRP_56' 'GRP_57' 'GRP_58' 'GRP_59'\n",
            " 'GRP_6' 'GRP_60' 'GRP_61' 'GRP_62' 'GRP_63' 'GRP_64' 'GRP_65' 'GRP_66'\n",
            " 'GRP_67' 'GRP_68' 'GRP_69' 'GRP_7' 'GRP_70' 'GRP_71' 'GRP_72' 'GRP_73'\n",
            " 'GRP_8' 'GRP_9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YsHK5fqgYlki",
        "colab": {}
      },
      "source": [
        "# Importing necessary libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pprint, pickle\n",
        "import warnings, pickle, inflect, re\n",
        "import os, sys, itertools, string\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import nltk\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Eqe_0V3cID5e",
        "colab": {}
      },
      "source": [
        "# loading the tokenizer\n",
        "max_features = 50000\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer_in_use = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FI2XuqfCSlha",
        "outputId": "439519d9-408c-4e19-8bea-af8927fa0968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Downloading the required packages\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "56ffVrmFTFw1",
        "colab": {}
      },
      "source": [
        "# Initialize spacy 'en' large model, keeping only tagger component needed for lemmatization\n",
        "nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
        "# Tokenization using Regexp_tokenizer so that we get only words and digits in the token\n",
        "def tokenizer(text):\n",
        "  return regexp_tokenize(text,pattern=\"\\w+\")\n",
        "\n",
        "#Lemmatization Using Spacy\n",
        "def lemmatizer(sentence):\n",
        "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
        "    doc = nlp(sentence)\n",
        "    return \" \".join([token.lemma_ for token in doc if token.lemma_ !='-PRON-'])\n",
        "\n",
        "#stop words using NLTK\n",
        "def stopWordsRemoval():\n",
        "  mylist = stopwords.words('english')\n",
        "  # mylist = mylist.extend(['yes','na','hi','receive','hello','regards','thanks','from','greeting','will','please','see','help','able'])\n",
        "  return mylist\n",
        "  \n",
        "\n",
        "  # Extend the English Stop Words for the words imported from wordnet\n",
        "STOP_WORDS = STOPWORDS.union({'yes','na','hi',\n",
        "                              'receive','hello',\n",
        "                              'regards','thanks',\n",
        "                              'from','greeting',\n",
        "                              'forward','reply',\n",
        "                              'will','please',\n",
        "                              'see','help','able'})\n",
        "\n",
        "# Define regex patterns\n",
        "EMAIL_PATTERN = r\"([\\w.+-]+@[a-z\\d-]+\\.[a-z\\d.-]+)\"\n",
        "PUNCT_PATTERN = r\"[,|@|\\|?|\\\\|$&*|%|\\r|\\n|.:|\\s+|/|//|\\\\|/|\\||-|<|>|;|(|)|=|+|#|-|\\\"|[-\\]]|{|}]\"\n",
        "# Negative Lookbehind for EmailId replacement- Don't match any number which follows the text \"RetainedEmailId\"\n",
        "NUMER_PATTERN = r\"(?<!RetainedEmailId)(\\d+(?:\\.\\d+)?)\"\n",
        "def cleanseText(text):\n",
        "    # Make the text unicase (lower) \n",
        "    text = str(text).lower()\n",
        "    # Remove email adresses\n",
        "    # text = re.sub(EMAIL_PATTERN, '', text, flags=re.IGNORECASE)\n",
        "    # Save Email addresses and replace them with custom keyword\n",
        "    email_dict = extract_email(text)\n",
        "    for key in email_dict.keys():\n",
        "        text = text.replace(email_dict[key], key)\n",
        "    # Remove all numbers \n",
        "    text = re.sub(NUMER_PATTERN, '', text)\n",
        "    # Replace all punctuations with blank space\n",
        "    # text = re.sub(PUNCT_PATTERN, \" \", text, flags=re.MULTILINE)\n",
        "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Replace multiple spaces from prev step to single\n",
        "    text = re.sub(r' {2,}', \" \", text, flags=re.MULTILINE)\n",
        "    text = text.replace('`',\"'\")\n",
        "    # Replace the email ids back into their original position\n",
        "    for key in email_dict.keys():\n",
        "        text = text.replace(key, email_dict[key])\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "\n",
        "def extract_email(text):\n",
        "    # Replaces the email addresses with custom key word and \n",
        "    # save them into a dictionary for future use\n",
        "    unique_emailid = set(re.findall(EMAIL_PATTERN, text))\n",
        "    email_replacement = dict()\n",
        "    for idx, email in enumerate(unique_emailid):\n",
        "        email_replacement[f'RetainedEmailId{idx}'] = email\n",
        "    return email_replacement\n",
        "  # This method callsthe cleanseTest, tokenizer and the lemmatization function\n",
        "\n",
        "def preProcessData(text):\n",
        "  cleansedText = cleanseText(text)\n",
        "  tokenized = tokenizer(cleansedText)\n",
        "  lemmatizedText = lemmatizer(\" \".join(word for word in tokenized))\n",
        "  return lemmatizedText\n",
        "  \n",
        "stwords = stopWordsRemoval()\n",
        "STOP_WORDS.union(set(stwords))\n",
        "STOP_WORDS.remove('no')\n",
        "STOP_WORDS.remove('not')\n",
        "# function to remove all the stopwords\n",
        "def removeStopWords(text):\n",
        "  return \" \".join([words.strip() for words in text.split() if words.strip() not in list(STOP_WORDS)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3SpAsEIgcjYK",
        "outputId": "9d81fecb-c860-4b22-ccac-2baa25350b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "# Insalling anvil-uplink\n",
        "!pip install anvil-uplink"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting anvil-uplink\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/5c/6674e25d83936fdf162826d2a9173c831f6c706fdcb30b1aa3b1e0632dcd/anvil_uplink-0.3.29-py2.py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from anvil-uplink) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Collecting argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Collecting ws4py==0.3.4\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/60/5d135c8161a2a67d7c227d57bb599fad967d818dbcdca08daa2d60eb87b9/ws4py-0.3.4.tar.gz\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.3.4-cp36-none-any.whl size=41809 sha256=c97f47edb930ff272da017833242c5b3aaa5708f1260b8ad6507cc29c93f35b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/1f/0d/beff5822af761b66067b5e0b251a9c66af3ae15828ee9a8f15\n",
            "Successfully built ws4py\n",
            "Installing collected packages: argparse, ws4py, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.3.29 argparse-1.4.0 ws4py-0.3.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ctuQQvnAk3gR",
        "outputId": "17be1341-7962-434a-e4f4-ce75e7e4befd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#Connecting to the server\n",
        "import anvil.server\n",
        "\n",
        "anvil.server.connect(\"RAJVFTEHMJWRR2MFQKYI4EAF-2CSMYUKQ544AGBV6\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Authenticated OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f2za_q-0lI3V",
        "colab": {}
      },
      "source": [
        "# function to predict the group\n",
        "\n",
        "@anvil.server.callable\n",
        "def predict_group(shortDescription,description):\n",
        "  if not shortDescription.strip():\n",
        "    shortDescription=''\n",
        "  if not description.strip():\n",
        "    description=''\n",
        "  combinedDescription =  \" \".join([shortDescription, description]).strip()\n",
        "  combinedDescription = preProcessData(combinedDescription)\n",
        "  combinedDescription = removeStopWords(combinedDescription)\n",
        "  combinedDescription=[combinedDescription]\n",
        "  print(combinedDescription)\n",
        "  seq = tokenizer_in_use.texts_to_sequences(combinedDescription)\n",
        "  padded = pad_sequences(seq, maxlen=926)\n",
        "  from tensorflow.keras.models import load_model\n",
        "\n",
        "  dependencies = {\n",
        "      'precision' : precision,\n",
        "      'recall' : recall\n",
        "  }\n",
        "\n",
        "  model = load_model('model_emb_bilstm_sampledData.h5', custom_objects=dependencies)\n",
        "  pred = model.predict(padded)\n",
        "  return pred,labels[np.argmax(pred)]\n",
        "anvil.server.wait_forever()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4hLdbz2ZAtkM"
      },
      "source": [
        "The link to our webApp is :\n",
        "https://nlpgroup4automaticticketclassificationsystem.anvil.app/"
      ]
    }
  ]
}