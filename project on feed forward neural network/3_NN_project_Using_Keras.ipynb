{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDGFtlEkdUe9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "77t7bnZ1-VbO",
    "outputId": "96111c05-7ee2-4ad3-bc7e-8c64d32866f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mounting the drive where the dataset is present\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMGP6XMi-d4d"
   },
   "outputs": [],
   "source": [
    "# Getting the dataset from the drive\n",
    "import h5py\n",
    "filepath = '/content/drive/My Drive/Deep Learning/SVHN_single_grey1.h5'\n",
    "h5f = h5py.File(filepath, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPf2pupB_lv7"
   },
   "outputs": [],
   "source": [
    "#Load the training and test set\n",
    "\n",
    "X_train_original = h5f['X_train'][:]\n",
    "y_train_original = h5f['y_train'][:]\n",
    "X_test_original  = h5f['X_test'][:]\n",
    "y_test_original  = h5f['y_test'][:]\n",
    "X_val_original   =h5f['X_val'][:]\n",
    "y_val_original   =h5f['y_val'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "3Wk0te3E2SvD",
    "outputId": "ebe7edf3-a633-4c7e-8372-f52e466f6214"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "FLQOaO392MgL",
    "outputId": "487e57b1-714f-47bb-e3cd-d6933b1e4cdd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7EIkwv8HBWiFlhK7ycwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  2\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train_original[0])    # show first element in the dataset\n",
    "plt.show()\n",
    "print('Label: ', y_train_original[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "BHM202xg2enj",
    "outputId": "9235c45e-0bee-462e-8063-1e4b5b4d9eb7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  8\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_test_original[10])    # show first number in the dataset\n",
    "plt.show()\n",
    "print('Label: ', y_test_original[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-epDadsvVOF"
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YjlE6oFj25gb",
    "outputId": "48cc518c-99b5-4fa1-b1e2-3cc383a8ec1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 32, 32) (18000, 32, 32) (42000,) (18000,) (60000, 32, 32) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_original.shape, X_test_original.shape, y_train_original.shape, y_test_original.shape,X_val_original.shape,y_val_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FkJpypiq2_9s",
    "outputId": "0bef7004-ed2d-4ea1-b081-7f6f53f9f01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024) (18000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# reshaping X data: (n, 32, 32) => (n, 1024)\n",
    "X_train = X_train_original.reshape((X_train_original.shape[0], -1))\n",
    "X_test = X_test_original.reshape((X_test_original.shape[0], -1))\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnpv447e62to"
   },
   "outputs": [],
   "source": [
    "# normalising inputs from 0-255 to 0-1\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPVjYXTr4E7T"
   },
   "outputs": [],
   "source": [
    "# converting y data into categorical (one-hot encoding)\n",
    "y_train = to_categorical(y_train_original)\n",
    "y_test = to_categorical(y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eXEtPz7T4VBj",
    "outputId": "6df64d70-5f86-481f-abf0-759e6a4f2df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024) (18000, 1024) (42000, 10) (18000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "824wXp3M4fje"
   },
   "source": [
    "### Basic NN model\n",
    "\n",
    "Naive MLP model without any alterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFfJ55T04iPq"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "4EhBirM74tZL",
    "outputId": "b44649c2-b8de-4de2-8613-11006196dd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "0D7xsBio4ziL",
    "outputId": "7b2373d3-badf-4001-eb73-af889b5eb403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(50, input_shape = (1024, )))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "PSGVpgpA46uK",
    "outputId": "518a6777-abfc-4e73-fa0d-2a1cfbacc659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BKOSqDcG4-Q8",
    "outputId": "228bb5cc-60f9-4d1e-acd6-2184b0bf6672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "42000/42000 [==============================] - 2s 47us/step - loss: 2.3617 - acc: 0.1003\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.0960\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3029 - acc: 0.0970\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0986\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0989\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0992\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0988\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0988\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0996\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0993\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0995\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0983\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0983\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0986\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0996\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0991\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3029 - acc: 0.0992\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1002\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0981\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0986\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.1000\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0998\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1001\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0987\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.1002\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3029 - acc: 0.0985\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1012\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0995\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0997\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0984\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0993\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0980\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0997\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0982\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1009\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1018\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0987\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0981\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.1000\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0999\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0987\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0999\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1015\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.0987\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1003\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1002\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.0988\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0995\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0977\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1012\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0993\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0983\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0995\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0997\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1013\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1003\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0998\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0994\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0976\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1010\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1002\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3029 - acc: 0.0999\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1003\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0997\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0998\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1004\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1000\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1008\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3027 - acc: 0.0990\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.1002\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0983\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.1002\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0993\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0999\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0979\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1025\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0974\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0981\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1001\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0988\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0992\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0985\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0985\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0982\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0981\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0979\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0999\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.1002\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1004\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1007\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1011\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1013\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3029 - acc: 0.0995\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1001\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0985\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0995\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1012\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0989\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1017\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oXk0sddb5AeL",
    "outputId": "047aa63b-0014-4326-94b3-0e61701adcc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 37us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aW1t65Kn5YTj",
    "outputId": "2f7dd4e7-4919-4ed9-bb6e-7333eaa71614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.10038888888888889\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "tYfcrsroHq3X",
    "outputId": "5e052c33-229c-478b-dfbd-e1d07ac20b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.10038888888888889\n",
      "----------Confusion Matrix----------\n",
      "[[   0    0 1770    0    0   44    0    0    0    0]\n",
      " [   0    0 1798    0    0   30    0    0    0    0]\n",
      " [   0    0 1766    0    0   37    0    0    0    0]\n",
      " [   0    0 1694    0    0   25    0    0    0    0]\n",
      " [   0    0 1790    0    0   22    0    0    0    0]\n",
      " [   0    0 1727    0    0   41    0    0    0    0]\n",
      " [   0    0 1783    0    0   49    0    0    0    0]\n",
      " [   0    0 1758    0    0   50    0    0    0    0]\n",
      " [   0    0 1772    0    0   40    0    0    0    0]\n",
      " [   0    0 1765    0    0   39    0    0    0    0]]\n",
      "--------Recall and Precision for multiclass--------\n",
      "Recall :  [0.         0.         0.97947865 0.         0.         0.02319005\n",
      " 0.         0.         0.         0.        ]\n",
      "Precision :  [       nan        nan 0.10020995        nan        nan 0.10875332\n",
      "        nan        nan        nan        nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_test)\n",
    "classes = np.argmax(ypred, axis=1)\n",
    "# calculate accuracy\n",
    "print(\"Accuracy of the model is : \",metrics.accuracy_score(y_test_original, classes))\n",
    "# examine the class distribution of the testing set\n",
    "print(\"----------Confusion Matrix----------\")\n",
    "cm = metrics.confusion_matrix(y_test_original, classes)\n",
    "print(cm)\n",
    "print(\"--------Recall and Precision for multiclass--------\")\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"Precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_5lhODQySH7"
   },
   "source": [
    "### Weight Initialization\n",
    "\n",
    "# Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2VO6Ln5E_Bz"
   },
   "outputs": [],
   "source": [
    "# from now on, create a function to generate (return) models\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))     # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.01)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kxZhAtwCFEE6",
    "outputId": "c3471e29-1e51-42ab-ec9d-149cf24412de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 21us/step - loss: 2.3381 - acc: 0.0992\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0995\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0982\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1005\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0961\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0977\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0996\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0990\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1000\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0980\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0986\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0965\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0998\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1001\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0989\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0988\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1002\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1009\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0999\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0989\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1003\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0995\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0994\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0992\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0987\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0984\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1003\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0997\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0982\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0994\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0990\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1012\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0990\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1005\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1008\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0999\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0979\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1004\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0977\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0989\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0995\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0993\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1001\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1006\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1001\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1002\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0984\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1009\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1000\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1004\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1004\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1012\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1004\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0982\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1010\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0990\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1000\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0998\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3027 - acc: 0.0978\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1007\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1005\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0986\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0994\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1003\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1002\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0995\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0996\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1012\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0990\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1005\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.0998\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1013\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0996\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1004\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0982\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0998\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1037\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.0979\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0989\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0994\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1000\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1014\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0997\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0976\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1014\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0998\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 2.3028 - acc: 0.0982\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0990\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3028 - acc: 0.0994\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3027 - acc: 0.0996\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1001\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1004\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1002\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0995\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0994\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0994\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0996\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1009\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1018\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.3027 - acc: 0.0990\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4JZaJuAxFc3x",
    "outputId": "b2d37259-7f55-4aae-a62f-15c213798cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p1fWTWspHzIv",
    "outputId": "b8f66e79-1ec4-4aaa-b0c4-6c37d6f75765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.09994444444444445\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "ngljxFteOIH9",
    "outputId": "db0cc610-bfc2-49b6-c000-4e1a2d18639e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.09994444444444445\n",
      "----------Confusion Matrix----------\n",
      "[[   0    0    0    0    0  828    0    0    0  986]\n",
      " [  16    0    0    0    0  832    0    0    0  980]\n",
      " [  10    0    0    0    2  739    0    0    0 1052]\n",
      " [   2    0    0    0    0  834    0    0    0  883]\n",
      " [   1    0    0    0    0  765    0    0    0 1046]\n",
      " [   0    0    0    0    1  817    0    0    0  950]\n",
      " [   0    0    0    0    0  822    0    0    0 1010]\n",
      " [  21    0    0    0    2  810    0    0    0  975]\n",
      " [   1    0    0    0    1  796    0    0    0 1014]\n",
      " [   3    0    0    0    1  818    0    0    0  982]]\n",
      "--------Recall and Precision for multiclass--------\n",
      "Recall :  [0.         0.         0.         0.         0.         0.46210407\n",
      " 0.         0.         0.         0.5443459 ]\n",
      "Precision :  [0.                nan        nan        nan 0.         0.10135219\n",
      "        nan        nan        nan 0.09941284]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_test)\n",
    "classes = np.argmax(ypred, axis=1)\n",
    "# calculate accuracy\n",
    "print(\"Accuracy of the model is : \",metrics.accuracy_score(y_test_original, classes))\n",
    "# examine the class distribution of the testing set\n",
    "print(\"----------Confusion Matrix----------\")\n",
    "cm = metrics.confusion_matrix(y_test_original, classes)\n",
    "print(cm)\n",
    "print(\"--------Recall and Precision for multiclass--------\")\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"Precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4PIPaGlH8Ks"
   },
   "source": [
    "### Nonlinearity (Activation function)\n",
    "\n",
    "Sigmoid functions suffer from gradient vanishing problem, making training slower\n",
    "\n",
    "There are many choices apart from sigmoid and tanh; try many of them!\n",
    "\n",
    "'relu' (rectified linear unit) is one of the most popular ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TnU2DRvLH0uB"
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (1024, )))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.01)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vvLejjQCH3yl",
    "outputId": "e1962fba-7f36-4fc7-a18b-cc961a3de834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 22us/step - loss: 2.3045 - acc: 0.1098\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.2998 - acc: 0.1164\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2976 - acc: 0.1285\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 2.2948 - acc: 0.1432\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2903 - acc: 0.1617\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2841 - acc: 0.1739\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 2.2754 - acc: 0.1862\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2627 - acc: 0.2009\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2446 - acc: 0.2173\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2178 - acc: 0.2389\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.1776 - acc: 0.2547\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.1214 - acc: 0.2696\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 2.0537 - acc: 0.2903\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.9837 - acc: 0.3205\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.9159 - acc: 0.3420\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.8615 - acc: 0.3597\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.8273 - acc: 0.3672\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.7831 - acc: 0.3865\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.7495 - acc: 0.3969\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.7081 - acc: 0.4197\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.6657 - acc: 0.4313\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.6324 - acc: 0.4470\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.5858 - acc: 0.4677\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.5565 - acc: 0.4763\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.5173 - acc: 0.4956\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 1.4911 - acc: 0.5038\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.4559 - acc: 0.5164\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.4343 - acc: 0.5256\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.4032 - acc: 0.5378\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.3791 - acc: 0.5500\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.3662 - acc: 0.5548\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.3410 - acc: 0.5593\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.3099 - acc: 0.5794\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.3038 - acc: 0.5758\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.2920 - acc: 0.5825\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.2620 - acc: 0.5950\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 1.2499 - acc: 0.5993\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.2353 - acc: 0.6052\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.2123 - acc: 0.6133\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.2033 - acc: 0.6161\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.1845 - acc: 0.6243\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.1745 - acc: 0.6273\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.1627 - acc: 0.6326\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.1390 - acc: 0.6408\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.1375 - acc: 0.6415\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.1290 - acc: 0.6422\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.1190 - acc: 0.6473\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 1.1063 - acc: 0.6535\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 1.1037 - acc: 0.6544\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0898 - acc: 0.6606\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.0840 - acc: 0.6594\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0677 - acc: 0.6654\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0572 - acc: 0.6688\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0534 - acc: 0.6694\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.0508 - acc: 0.6707\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0425 - acc: 0.6749\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0285 - acc: 0.6802\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.0202 - acc: 0.6824\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0124 - acc: 0.6849\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.0089 - acc: 0.6868\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.0013 - acc: 0.6890\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9939 - acc: 0.6910\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.9854 - acc: 0.6933\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9773 - acc: 0.6968\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.9712 - acc: 0.6998\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9599 - acc: 0.7021\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9592 - acc: 0.7016\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9504 - acc: 0.7048\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9397 - acc: 0.7082\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9359 - acc: 0.7111\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9332 - acc: 0.7121\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.9276 - acc: 0.7127\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9156 - acc: 0.7173\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9105 - acc: 0.7201\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9110 - acc: 0.7197\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8967 - acc: 0.7239\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8968 - acc: 0.7236\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8938 - acc: 0.7238\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.8815 - acc: 0.7279\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8787 - acc: 0.7293\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8760 - acc: 0.7295\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8744 - acc: 0.7310\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8630 - acc: 0.7345\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8592 - acc: 0.7355\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8554 - acc: 0.7370\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8520 - acc: 0.7387\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8372 - acc: 0.7429\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8432 - acc: 0.7394\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8322 - acc: 0.7443\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8400 - acc: 0.7424\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8327 - acc: 0.7434\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8231 - acc: 0.7470\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.8240 - acc: 0.7457\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8204 - acc: 0.7469\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8144 - acc: 0.7517\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8124 - acc: 0.7495\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8128 - acc: 0.7499\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8051 - acc: 0.7530\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8000 - acc: 0.7525\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.7967 - acc: 0.7547\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zxZed-ahIFiK",
    "outputId": "83382534-6c7b-40d2-8706-12ebaa1f5b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 41us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7pYfkz0EIsIh",
    "outputId": "e55d3ef4-adcc-4933-fce3-464e31a3fe16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7476666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "iQfunduEOamD",
    "outputId": "88793364-69fc-4645-f742-228c36809be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.7476666666666667\n",
      "----------Confusion Matrix----------\n",
      "[[1405   43   20   41   48   25   74   33   64   61]\n",
      " [  31 1511   29   61   68   17   14   46   29   22]\n",
      " [  10   55 1386   63   31   28    9  148   36   37]\n",
      " [  16  113   54 1212   13  159   16   77   40   19]\n",
      " [  35   95   42   29 1443   28   52   14   46   28]\n",
      " [  25   63   21  171   38 1253   51   38   73   35]\n",
      " [  70   48   23   32  104   85 1310    9  134   17]\n",
      " [  18   84  109   48    8   24   12 1453   26   26]\n",
      " [  48   74   40   71   58   81  101   23 1248   68]\n",
      " [  88   87   56   89   38   74   11   43   81 1237]]\n",
      "--------Recall and Precision for multiclass--------\n",
      "Recall :  [0.77453142 0.82658643 0.7687188  0.70506108 0.79635762 0.70871041\n",
      " 0.7150655  0.80365044 0.68874172 0.68569845]\n",
      "Precision :  [0.80469645 0.69535205 0.77865169 0.66703357 0.78042185 0.70631342\n",
      " 0.79393939 0.77123142 0.70230726 0.79806452]\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_test)\n",
    "classes = np.argmax(ypred, axis=1)\n",
    "# calculate accuracy\n",
    "print(\"Accuracy of the model is : \",metrics.accuracy_score(y_test_original, classes))\n",
    "# examine the class distribution of the testing set\n",
    "print(\"----------Confusion Matrix----------\")\n",
    "cm = metrics.confusion_matrix(y_test_original, classes)\n",
    "print(cm)\n",
    "print(\"--------Recall and Precision for multiclass--------\")\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"Precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vh1c9QaKIx39"
   },
   "source": [
    "### Nonlinearity (Activation function) And Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GG6bGg01IuLB"
   },
   "outputs": [],
   "source": [
    "# from now on, create a function to generate (return) models\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))     # use he_normal initializer\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.01)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5s8sIQJJIwwt",
    "outputId": "e8cde030-a765-4405-ffc9-e08ed7324efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3004 - acc: 0.1179\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2792 - acc: 0.1483\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2525 - acc: 0.1813\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 2.2054 - acc: 0.2304\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 2.1191 - acc: 0.2850\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.9998 - acc: 0.3263\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.8853 - acc: 0.3655\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.7865 - acc: 0.4059\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.6904 - acc: 0.4481\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.5968 - acc: 0.4852\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.5077 - acc: 0.5167\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.4392 - acc: 0.5385\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.3841 - acc: 0.5570\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.3379 - acc: 0.5748\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.2920 - acc: 0.5883\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.2523 - acc: 0.6017\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 1.2329 - acc: 0.6127\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.2057 - acc: 0.6210\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.1840 - acc: 0.6312\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.1554 - acc: 0.6405\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.1414 - acc: 0.6443\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.1086 - acc: 0.6550\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 1.1042 - acc: 0.6586\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0931 - acc: 0.6617\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0697 - acc: 0.6696\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.0540 - acc: 0.6755\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0552 - acc: 0.6722\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 1.0291 - acc: 0.6798\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.0201 - acc: 0.6866\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 1.0057 - acc: 0.6938\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9959 - acc: 0.6946\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9844 - acc: 0.6959\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9763 - acc: 0.7008\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.9660 - acc: 0.7035\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9591 - acc: 0.7044\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9462 - acc: 0.7085\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 0.9443 - acc: 0.7102\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9339 - acc: 0.7135\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9293 - acc: 0.7147\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9120 - acc: 0.7197\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.9056 - acc: 0.7238\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.9034 - acc: 0.7230\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.9012 - acc: 0.7236\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8842 - acc: 0.7294\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.8748 - acc: 0.7338\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8723 - acc: 0.7322\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8734 - acc: 0.7322\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8624 - acc: 0.7382\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8578 - acc: 0.7380\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8485 - acc: 0.7410\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8508 - acc: 0.7410\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8398 - acc: 0.7426\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8347 - acc: 0.7460\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8331 - acc: 0.7460\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.8232 - acc: 0.7488\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.8237 - acc: 0.7503\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8098 - acc: 0.7530\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8074 - acc: 0.7539\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8038 - acc: 0.7547\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8026 - acc: 0.7557\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.8013 - acc: 0.7556\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7924 - acc: 0.7580\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7866 - acc: 0.7606\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7825 - acc: 0.7595\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7790 - acc: 0.7619\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7772 - acc: 0.7644\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7666 - acc: 0.7678\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7626 - acc: 0.7687\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7630 - acc: 0.7693\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7596 - acc: 0.7702\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7595 - acc: 0.7687\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7542 - acc: 0.7704\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7489 - acc: 0.7699\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.7444 - acc: 0.7736\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.7414 - acc: 0.7727\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7412 - acc: 0.7729\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: 0.7330 - acc: 0.7776\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7322 - acc: 0.7761\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7322 - acc: 0.7779\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7283 - acc: 0.7805\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7269 - acc: 0.7787\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7204 - acc: 0.7799\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7178 - acc: 0.7824\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7138 - acc: 0.7838\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7087 - acc: 0.7858\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7089 - acc: 0.7845\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7017 - acc: 0.7860\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.7061 - acc: 0.7860\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.6976 - acc: 0.7889\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.7016 - acc: 0.7864\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.6957 - acc: 0.7886\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.6913 - acc: 0.7889\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.6933 - acc: 0.7907\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.6881 - acc: 0.7928\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.6835 - acc: 0.7930\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 18us/step - loss: 0.6806 - acc: 0.7929\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.6805 - acc: 0.7927\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.6746 - acc: 0.7947\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 17us/step - loss: 0.6729 - acc: 0.7946\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 19us/step - loss: 0.6773 - acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h1DPXdWFI5XR",
    "outputId": "480e815f-56cd-4260-8b4a-a056f0050600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 43us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AjsjKU3cJMzB",
    "outputId": "20cab6fc-073d-46a0-ea4f-3ca27ba6cc9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7675555555555555\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "pcPXb_kXOdUE",
    "outputId": "10822ce5-13a4-4e5f-984b-a1e04e8c1704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.7675555555555555\n",
      "----------Confusion Matrix----------\n",
      "[[1374  114   33   33   51   11   46   68   26   58]\n",
      " [  18 1602   30   27   38   10   11   65   16   11]\n",
      " [   9   73 1424   42   30   12   14  151   21   27]\n",
      " [  17   91   59 1298   14  107    9   62   37   25]\n",
      " [  37  136   35   27 1467   14   25   24   18   29]\n",
      " [  19   64   23  160   26 1299   46   28   60   43]\n",
      " [  45   69   24   32   81   95 1340   20  112   14]\n",
      " [  16  108   64   19    8    8    9 1546    9   21]\n",
      " [  34   91   67   84   46   95   73   20 1239   63]\n",
      " [  70  114   59   74   49   51   15   64   81 1227]]\n",
      "--------Recall and Precision for multiclass--------\n",
      "Recall :  [0.75744212 0.87636761 0.78979479 0.75509017 0.80960265 0.73472851\n",
      " 0.73144105 0.8550885  0.68377483 0.68015521]\n",
      "Precision :  [0.83831605 0.6506905  0.78327833 0.72271715 0.81049724 0.76321974\n",
      " 0.84382872 0.75488281 0.76528721 0.8083004 ]\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_test)\n",
    "classes = np.argmax(ypred, axis=1)\n",
    "# calculate accuracy\n",
    "print(\"Accuracy of the model is : \",metrics.accuracy_score(y_test_original, classes))\n",
    "# examine the class distribution of the testing set\n",
    "print(\"----------Confusion Matrix----------\")\n",
    "cm = metrics.confusion_matrix(y_test_original, classes)\n",
    "print(cm)\n",
    "print(\"--------Recall and Precision for multiclass--------\")\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"Precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpLNGn1hJSd9"
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective\n",
    "\n",
    "Normalize each mini-batch before nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksedEhM0JPyZ"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-SrNVPlJXlY"
   },
   "source": [
    "Batch normalization layer is usually inserted after dense/convolution and before nonlinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rs7K1fQzJVIR"
   },
   "outputs": [],
   "source": [
    "def bn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (1024, )))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.01)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TyA--K7aJcCo",
    "outputId": "327ce941-859f-4610-b674-19887020d91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 2s 56us/step - loss: 2.2709 - acc: 0.1872\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 1.8646 - acc: 0.3649\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 1.5954 - acc: 0.4847\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 1.4090 - acc: 0.5597\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 1.2746 - acc: 0.6061\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 1.1781 - acc: 0.6360\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 1.1066 - acc: 0.6562\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 1.0528 - acc: 0.6730\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 1.0026 - acc: 0.6872\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 2s 41us/step - loss: 0.9635 - acc: 0.7005\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.9326 - acc: 0.7107\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.9044 - acc: 0.7181\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.8801 - acc: 0.7252\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.8500 - acc: 0.7346\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.8336 - acc: 0.7402\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.8121 - acc: 0.7474\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.7906 - acc: 0.7528\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.7817 - acc: 0.7558\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.7631 - acc: 0.7601\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.7481 - acc: 0.7667\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.7370 - acc: 0.7698\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.7277 - acc: 0.7729\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.7177 - acc: 0.7777\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.7021 - acc: 0.7804\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.6972 - acc: 0.7831\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.6866 - acc: 0.7863\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.6764 - acc: 0.7885\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.6679 - acc: 0.7916\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 2s 42us/step - loss: 0.6580 - acc: 0.7939\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 2s 42us/step - loss: 0.6528 - acc: 0.7961\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.6419 - acc: 0.8009\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.6355 - acc: 0.8024\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.6323 - acc: 0.8025\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.6236 - acc: 0.8042\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.6200 - acc: 0.8083\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.6131 - acc: 0.8100\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.6039 - acc: 0.8115\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.6000 - acc: 0.8144\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5900 - acc: 0.8160\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5881 - acc: 0.8178\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5849 - acc: 0.8177\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5780 - acc: 0.8195\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5767 - acc: 0.8212\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5689 - acc: 0.8231\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5658 - acc: 0.8247\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5650 - acc: 0.8244\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5555 - acc: 0.8269\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5531 - acc: 0.8277\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5489 - acc: 0.8290\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5483 - acc: 0.8289\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5387 - acc: 0.8311\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5391 - acc: 0.8315\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5345 - acc: 0.8334\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5308 - acc: 0.8339\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5282 - acc: 0.8350\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5233 - acc: 0.8373\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5247 - acc: 0.8365\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5192 - acc: 0.8379\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5136 - acc: 0.8398\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5078 - acc: 0.8412\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.5058 - acc: 0.8429\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5090 - acc: 0.8413\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.5054 - acc: 0.8408\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.5023 - acc: 0.8435\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4953 - acc: 0.8455\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 2s 37us/step - loss: 0.4902 - acc: 0.8474\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4912 - acc: 0.8450\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4919 - acc: 0.8461\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4832 - acc: 0.8481\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4802 - acc: 0.8485\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4801 - acc: 0.8494\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4770 - acc: 0.8506\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4740 - acc: 0.8506\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4721 - acc: 0.8531\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4725 - acc: 0.8533\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4706 - acc: 0.8516\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4650 - acc: 0.8541\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4640 - acc: 0.8566\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4652 - acc: 0.8545\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4574 - acc: 0.8559\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4540 - acc: 0.8574\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.4581 - acc: 0.8571\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 2s 41us/step - loss: 0.4570 - acc: 0.8556\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4501 - acc: 0.8587\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4486 - acc: 0.8586\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4509 - acc: 0.8585\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.4453 - acc: 0.8598\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4439 - acc: 0.8618\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4453 - acc: 0.8609\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4400 - acc: 0.8620\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4384 - acc: 0.8623\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.4382 - acc: 0.8619\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4341 - acc: 0.8641\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4348 - acc: 0.8625\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4311 - acc: 0.8642\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4304 - acc: 0.8658\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4314 - acc: 0.8639\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4276 - acc: 0.8657\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.4225 - acc: 0.8666\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 2s 39us/step - loss: 0.4261 - acc: 0.8661\n"
     ]
    }
   ],
   "source": [
    "model = bn_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f7IukRfRJgxR",
    "outputId": "4f1e3b60-debd-4e2f-8755-374f10a8009e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 68us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x13PaYqaKKDr",
    "outputId": "5d2ba519-dae6-4983-a6d5-aee88d29e470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.781\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "STbrx6rRR9Rg",
    "outputId": "3190906e-ac2e-4927-9b0f-c03e956aa2ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.781\n",
      "----------Confusion Matrix----------\n",
      "[[1476   24   13   20   22   20  133   14   23   69]\n",
      " [  48 1351   30   68   50   30   87   97   44   23]\n",
      " [  32    7 1351  111   35   29   37   75   66   60]\n",
      " [  16   17   24 1291   13  180   49   27   73   29]\n",
      " [  41   35   32   54 1314   42  175   14   51   54]\n",
      " [   7   11    5   71   10 1446  132    5   40   41]\n",
      " [  24   12    7   23   14   83 1585    8   60   16]\n",
      " [  18   36   40   61   14   25   37 1535   22   20]\n",
      " [  30   15   15   60   16   65  183   13 1362   53]\n",
      " [  79   20   24   51   18  106   59   23   77 1347]]\n",
      "--------Recall and Precision for multiclass--------\n",
      "Recall :  [0.81367144 0.73905908 0.74930671 0.75101803 0.72516556 0.8178733\n",
      " 0.86517467 0.84900442 0.75165563 0.74667406]\n",
      "Precision :  [0.83342744 0.8841623  0.87670344 0.71325967 0.87250996 0.71372162\n",
      " 0.63988696 0.84759801 0.74917492 0.78679907]\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_test)\n",
    "classes = np.argmax(ypred, axis=1)\n",
    "# calculate accuracy\n",
    "print(\"Accuracy of the model is : \",metrics.accuracy_score(y_test_original, classes))\n",
    "# examine the class distribution of the testing set\n",
    "print(\"----------Confusion Matrix----------\")\n",
    "cm = metrics.confusion_matrix(y_test_original, classes)\n",
    "print(cm)\n",
    "print(\"--------Recall and Precision for multiclass--------\")\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"Precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfUNkdNKKexM"
   },
   "source": [
    "# Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBmE03cxKZf5"
   },
   "outputs": [],
   "source": [
    "def dropout_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (1024, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.001)\n",
    "    sgd = optimizers.SGD(lr = 0.01)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T3VJpN-qKdbf",
    "outputId": "65df5803-51e8-49f2-ed19-0fce72830fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 2.3041 - acc: 0.0995\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3027 - acc: 0.1002\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3027 - acc: 0.1012\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3027 - acc: 0.0979\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1006\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3027 - acc: 0.1015\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.1007\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3027 - acc: 0.1017\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3027 - acc: 0.1009\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3027 - acc: 0.0999\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1010\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1008\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3027 - acc: 0.1014\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1012\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1008\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1013\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1014\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1010\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.0993\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1011\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1012\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3027 - acc: 0.1019\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 2.3026 - acc: 0.1009\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1007\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.1013\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1004\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1014\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1006\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1012\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.0995\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1001\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3027 - acc: 0.0999\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1013\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1011\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1013\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1017\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.0991\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1008\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1010\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1011\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1015\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1003\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1004\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1009\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 2.3026 - acc: 0.1028\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 2.3026 - acc: 0.1020\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 2.3026 - acc: 0.1019\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 2.3026 - acc: 0.1010\n"
     ]
    }
   ],
   "source": [
    "model = dropout_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qc08lld9KmyJ",
    "outputId": "c8cb4767-85d4-4e57-8f87-bc3525ab6c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 52us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ysfH1RFQLDrh",
    "outputId": "938c2293-dd03-4a62-e579-b142afddd1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.0955\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "SAEKofRgLGUx",
    "outputId": "533c4782-7d84-4f47-d235-ec9667c483c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.0955\n",
      "----------Confusion Matrix----------\n",
      "[[   0    0    0 1814    0    0    0    0    0    0]\n",
      " [   0    0    0 1828    0    0    0    0    0    0]\n",
      " [   0    0    0 1803    0    0    0    0    0    0]\n",
      " [   0    0    0 1719    0    0    0    0    0    0]\n",
      " [   0    0    0 1812    0    0    0    0    0    0]\n",
      " [   0    0    0 1768    0    0    0    0    0    0]\n",
      " [   0    0    0 1832    0    0    0    0    0    0]\n",
      " [   0    0    0 1808    0    0    0    0    0    0]\n",
      " [   0    0    0 1812    0    0    0    0    0    0]\n",
      " [   0    0    0 1804    0    0    0    0    0    0]]\n",
      "--------Recall and Precision for multiclass--------\n",
      "Recall :  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Precision :  [   nan    nan    nan 0.0955    nan    nan    nan    nan    nan    nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_test)\n",
    "classes = np.argmax(ypred, axis=1)\n",
    "# calculate accuracy\n",
    "print(\"Accuracy of the model is : \",metrics.accuracy_score(y_test_original, classes))\n",
    "# examine the class distribution of the testing set\n",
    "print(\"----------Confusion Matrix----------\")\n",
    "cm = metrics.confusion_matrix(y_test_original, classes)\n",
    "print(cm)\n",
    "print(\"--------Recall and Precision for multiclass--------\")\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"Precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6wwnWXULJQv"
   },
   "source": [
    "### All techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VD7VuURqLJ8h"
   },
   "outputs": [],
   "source": [
    "def all_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.001)\n",
    "    sgd = optimizers.SGD(lr = 0.01)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a2ckP3t2LM4h",
    "outputId": "23c2a576-85e3-49bd-e6f6-996ba57e6bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 3s 80us/step - loss: 2.3727 - acc: 0.1441\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 1.9753 - acc: 0.2784\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.7201 - acc: 0.3882\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 1.5541 - acc: 0.4636\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 1.4380 - acc: 0.5129\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.3549 - acc: 0.5461\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.3001 - acc: 0.5731\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 1.2505 - acc: 0.5935\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.2042 - acc: 0.6099\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.1677 - acc: 0.6251\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 1.1340 - acc: 0.6400\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.1156 - acc: 0.6452\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 1.0983 - acc: 0.6527\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.0706 - acc: 0.6639\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 1.0722 - acc: 0.6640\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 1.0471 - acc: 0.6710\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 1.0317 - acc: 0.6788\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 1.0204 - acc: 0.6825\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 1.0077 - acc: 0.6878\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9974 - acc: 0.6885\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.9906 - acc: 0.6929\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9899 - acc: 0.6940\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9689 - acc: 0.7007\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9677 - acc: 0.7005\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9617 - acc: 0.7017\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.9534 - acc: 0.7038\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.9571 - acc: 0.7040\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.9487 - acc: 0.7049\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.9355 - acc: 0.7105\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.9511 - acc: 0.7056\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.9309 - acc: 0.7123\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.9336 - acc: 0.7110\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.9271 - acc: 0.7161\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.9251 - acc: 0.7127\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.9246 - acc: 0.7144\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.9222 - acc: 0.7146\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9177 - acc: 0.7172\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9100 - acc: 0.7187\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.9126 - acc: 0.7165\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.9062 - acc: 0.7194\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.9026 - acc: 0.7213\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.9005 - acc: 0.7203\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.8988 - acc: 0.7222\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8945 - acc: 0.7232\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8958 - acc: 0.7248\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8888 - acc: 0.7249\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 2s 49us/step - loss: 0.8909 - acc: 0.7254\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8865 - acc: 0.7249\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8787 - acc: 0.7292\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8818 - acc: 0.7296\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8807 - acc: 0.7271\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8851 - acc: 0.7271\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8777 - acc: 0.7317\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8726 - acc: 0.7295\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8726 - acc: 0.7317\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8644 - acc: 0.7335\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8703 - acc: 0.7316\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8707 - acc: 0.7316\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8620 - acc: 0.7335\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8664 - acc: 0.7335\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.8740 - acc: 0.7309\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8658 - acc: 0.7336\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8556 - acc: 0.7360\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8557 - acc: 0.7355\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8717 - acc: 0.7312\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8622 - acc: 0.7358\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8543 - acc: 0.7375\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8630 - acc: 0.7329\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8495 - acc: 0.7400\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8548 - acc: 0.7369\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8528 - acc: 0.7358\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8486 - acc: 0.7384\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8541 - acc: 0.7386\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8486 - acc: 0.7396\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8529 - acc: 0.7398\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8453 - acc: 0.7395\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8420 - acc: 0.7390\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8456 - acc: 0.7381\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8443 - acc: 0.7387\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8435 - acc: 0.7405\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.8461 - acc: 0.7392\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8302 - acc: 0.7454\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8369 - acc: 0.7433\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8430 - acc: 0.7404\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8441 - acc: 0.7410\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8372 - acc: 0.7442\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.8355 - acc: 0.7413\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.8364 - acc: 0.7433\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 2s 52us/step - loss: 0.8339 - acc: 0.7430\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.8320 - acc: 0.7456\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 2s 56us/step - loss: 0.8359 - acc: 0.7449\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8325 - acc: 0.7451\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 2s 56us/step - loss: 0.8283 - acc: 0.7447\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 2s 57us/step - loss: 0.8284 - acc: 0.7455\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 2s 55us/step - loss: 0.8274 - acc: 0.7451\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8354 - acc: 0.7433\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8229 - acc: 0.7466\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.8280 - acc: 0.7462\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 2s 55us/step - loss: 0.8284 - acc: 0.7461\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.8305 - acc: 0.7439\n"
     ]
    }
   ],
   "source": [
    "model = all_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zvtrcn4aLOzV",
    "outputId": "b8e187aa-dc56-45d2-e2fd-3c81a5b6e292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 83us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PPV3JvjvMH-z",
    "outputId": "c93207ba-2c25-416e-c555-9f2663254e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7742222222222223\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "MlPW-gTIO3uj",
    "outputId": "9c4dc1ba-61bb-4927-b4ae-b0a421bb4ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.7742222222222223\n",
      "----------Confusion Matrix----------\n",
      "[[1538   40   19   10   63    5   34   78    4   23]\n",
      " [  66 1456   26   14   97    7    4  141    8    9]\n",
      " [  27   43 1370   44   60    8    7  213   11   20]\n",
      " [  23   43   40 1269   65  119   11  114   18   17]\n",
      " [  25   71   30   12 1584    7   14   41    5   23]\n",
      " [  16   21   17  123   63 1368   59   55   22   24]\n",
      " [  74   43   14   19  111   77 1423   36   25   10]\n",
      " [  24   54   31   16   30    4    7 1633    0    9]\n",
      " [  59   56   44   82  101   78  193   55 1078   66]\n",
      " [ 128   49   47   59   84   48   15  128   29 1217]]\n",
      "--------Recall and Precision for multiclass--------\n",
      "Recall :  [0.84785006 0.79649891 0.7598447  0.7382199  0.87417219 0.77375566\n",
      " 0.77674672 0.90320796 0.59492274 0.67461197]\n",
      "Precision :  [0.77676768 0.7761194  0.83638584 0.77002427 0.70150576 0.79488669\n",
      " 0.80531975 0.65477145 0.89833333 0.85825106]\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_test)\n",
    "classes = np.argmax(ypred, axis=1)\n",
    "# calculate accuracy\n",
    "print(\"Accuracy of the model is : \",metrics.accuracy_score(y_test_original, classes))\n",
    "# examine the class distribution of the testing set\n",
    "print(\"----------Confusion Matrix----------\")\n",
    "cm = metrics.confusion_matrix(y_test_original, classes)\n",
    "print(cm)\n",
    "print(\"--------Recall and Precision for multiclass--------\")\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"Precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XrYbYOT6P3W"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "1) We build a basic neural network whose results are given below:-\n",
    "\n",
    "      1.1 Naive MLP model without any alterations:-  Test accuracy:  0.10038888888888889\n",
    "\n",
    "      1.2 Weight Initialization:- Test accuracy:-  0.09994444444444445\n",
    "\n",
    "      1.3 Nonlinearity (Activation function):-  Test accuracy:  0.7476666666666667\n",
    "\n",
    "      1.4 Nonlinearity (Activation function) And Weight Initialization:- Test accuracy:  0.7675555555555555\n",
    "\n",
    "      1.5 Batch Normalization:- Test accuracy:  0.781\n",
    "  \n",
    "      1.6 Dropout:- Test accuracy:  0.0955\n",
    "\n",
    "      1.7 All techniques:- Test accuracy:  0.7742222222222223\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Overall Conclusion-----\n",
    "\n",
    "1) The SVHN_single_grey1 dataset is analysed in three ways - using KNN algorithm, using Feedforward neural network by building it from scratch and by using Keras. So we have three .ipynb files and three .htmnl files.\n",
    "\n",
    "2) On analysing the dataset we found that model build using Neural network gave us better result as compared to KNN.\n",
    "\n",
    "3) We are successful in finding the scores and confusion matrix for all the models we build.\n",
    "\n",
    "4) From all the models the best result we get from the Batch Normalization.\n",
    "\n",
    "5) For bettement of the model we need more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project_DL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
